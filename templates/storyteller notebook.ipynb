{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the functioning of the virtual game mastr chatbot program, written in Python.\n",
    "Let's start with an overview of the program then we will examine each section in detail.\n",
    "The purpose of the program is to use ChatGPT as a virtual game master that can take users through text-based adventure games on the web.\n",
    "This code is for a single player game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import os\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Environment variable for OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# AI Window Size - Number of moves the AI remembers\n",
    "AI_WINDOW_SIZE = 20\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a helpful bot named Bob.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template=\"{input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "# Initialize ChatOpenAI with the desired model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Initialize ConversationChain with the appropriate parameters\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    prompt=chat_prompt, \n",
    "    memory=ConversationBufferWindowMemory(k=AI_WINDOW_SIZE),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Render the chat interface\n",
    "    return render_template('chat.html')\n",
    "\n",
    "@app.route('/initiate_chat', methods=['GET'])\n",
    "def initiate_chat():\n",
    "    # Trigger initial AI response with empty user input\n",
    "    response = conversation.predict(input=\"\")\n",
    "    return jsonify({'reply': response})\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    user_input = request.json['message']\n",
    "    response = conversation.predict(input=user_input)\n",
    "    return jsonify({'reply': response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the header, which imports needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import os\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flask library is for running the python program with a web interface.\n",
    "The os library is for importing system variables.\n",
    "The langchain library is for interacting with LLMs.\n",
    "The langchain library is new, so let's talk about it a bit.\n",
    "\n",
    "langchain.chains.conversation.base.ConversationChain\n",
    "\n",
    "class langchain.chains.conversation.base.ConversationChain\n",
    "\n",
    "    Bases: LLMChain\n",
    "\n",
    "    Chain to have a conversation and load context from memory.\n",
    "\n",
    "    Example\n",
    "\n",
    "    from langchain.chains import ConversationChain\n",
    "    from langchain.llms import OpenAI\n",
    "\n",
    "    conversation = ConversationChain(llm=OpenAI())\n",
    "\n",
    "    Create a new model by parsing and validating input data from keyword arguments.\n",
    "\n",
    "    Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
    "\n",
    "    param callback_manager: Optional[BaseCallbackManager] = None\n",
    "\n",
    "        Deprecated, use callbacks instead.\n",
    "\n",
    "    param callbacks: Callbacks = None\n",
    "\n",
    "        Optional list of callback handlers (or callback manager). Defaults to None. Callback handlers are called throughout the lifecycle of a call to a chain, starting with on_chain_start, ending with on_chain_end or on_chain_error. Each custom chain can optionally call additional callback methods, see Callback docs for full details.\n",
    "\n",
    "    param llm: Union[Runnable[LanguageModelInput, str], Runnable[LanguageModelInput, BaseMessage]] [Required]\n",
    "\n",
    "        Language model to call.\n",
    "\n",
    "    param llm_kwargs: dict [Optional]\n",
    "\n",
    "    param memory: langchain.schema.memory.BaseMemory [Optional]\n",
    "\n",
    "        Default memory store.\n",
    "\n",
    "    param metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        Optional metadata associated with the chain. Defaults to None. This metadata will be associated with each call to this chain, and passed as arguments to the handlers defined in callbacks. You can use these to eg identify a specific instance of a chain with its use case.\n",
    "\n",
    "    param output_parser: BaseLLMOutputParser [Optional]\n",
    "\n",
    "        Output parser to use. Defaults to one that takes the most likely string but does not change it otherwise.\n",
    "\n",
    "    param prompt: langchain.schema.prompt_template.BasePromptTemplate = PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')\n",
    "\n",
    "        Default conversation prompt to use.\n",
    "\n",
    "    param return_final_only: bool = True\n",
    "\n",
    "        Whether to return only the final parsed result. Defaults to True. If false, will return a bunch of extra information about the generation.\n",
    "\n",
    "    param tags: Optional[List[str]] = None\n",
    "\n",
    "        Optional list of tags associated with the chain. Defaults to None. These tags will be associated with each call to this chain, and passed as arguments to the handlers defined in callbacks. You can use these to eg identify a specific instance of a chain with its use case.\n",
    "\n",
    "    param verbose: bool [Optional]\n",
    "\n",
    "        Whether or not run in verbose mode. In verbose mode, some intermediate logs will be printed to the console. Defaults to the global verbose value, accessible via langchain.globals.get_verbose().\n",
    "\n",
    "    __call__(inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) → Dict[str, Any]\n",
    "\n",
    "        Execute the chain.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                inputs – Dictionary of inputs, or single input if chain expects only one param. Should contain all inputs specified in Chain.input_keys except for inputs that will be set by the chain’s memory.\n",
    "\n",
    "                return_only_outputs – Whether to return only outputs in the response. If True, only new keys generated by this chain will be returned. If False, both input keys and new keys generated by this chain will be returned. Defaults to False.\n",
    "\n",
    "                callbacks – Callbacks to use for this chain run. These will be called in addition to callbacks passed to the chain during construction, but only these runtime callbacks will propagate to calls to other objects.\n",
    "\n",
    "                tags – List of string tags to pass to all callbacks. These will be passed in addition to tags passed to the chain during construction, but only these runtime tags will propagate to calls to other objects.\n",
    "\n",
    "                metadata – Optional metadata associated with the chain. Defaults to None\n",
    "\n",
    "                include_run_info – Whether to include run info in the response. Defaults to False.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            A dict of named outputs. Should contain all outputs specified in\n",
    "\n",
    "                Chain.output_keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain.memory.buffer_window.ConversationBufferWindowMemory\n",
    "\n",
    "class langchain.memory.buffer_window.ConversationBufferWindowMemory\n",
    "\n",
    "    Bases: BaseChatMemory\n",
    "\n",
    "    Buffer for storing conversation memory inside a limited size window.\n",
    "\n",
    "    Create a new model by parsing and validating input data from keyword arguments.\n",
    "\n",
    "    Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
    "\n",
    "    param ai_prefix: str = 'AI'\n",
    "\n",
    "    param chat_memory: BaseChatMessageHistory [Optional]\n",
    "\n",
    "    param human_prefix: str = 'Human'\n",
    "\n",
    "    param input_key: Optional[str] = None\n",
    "\n",
    "    param k: int = 5\n",
    "\n",
    "        Number of messages to store in buffer.\n",
    "\n",
    "    param output_key: Optional[str] = None\n",
    "\n",
    "    param return_messages: bool = False\n",
    "\n",
    "    clear() → None\n",
    "\n",
    "        Clear memory contents.\n",
    "\n",
    "    classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model\n",
    "\n",
    "        Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values\n",
    "\n",
    "    copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model\n",
    "\n",
    "        Duplicate a model, optionally choose which fields to include, exclude and change.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                include – fields to include in new model\n",
    "\n",
    "                exclude – fields to exclude from new model, as with values this takes precedence over include\n",
    "\n",
    "                update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n",
    "\n",
    "                deep – set to True to make a deep copy of the model\n",
    "\n",
    "        Returns\n",
    "\n",
    "            new model instance\n",
    "\n",
    "    dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) → DictStrAny\n",
    "\n",
    "        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
    "\n",
    "    classmethod from_orm(obj: Any) → Model\n",
    "\n",
    "    classmethod get_lc_namespace() → List[str]\n",
    "\n",
    "        Get the namespace of the langchain object.\n",
    "\n",
    "        For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [“langchain”, “llms”, “openai”]\n",
    "\n",
    "    classmethod is_lc_serializable() → bool\n",
    "\n",
    "        Is this class serializable?\n",
    "\n",
    "    json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode\n",
    "\n",
    "        Generate a JSON representation of the model, include and exclude arguments as per dict().\n",
    "\n",
    "        encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n",
    "\n",
    "    classmethod lc_id() → List[str]\n",
    "\n",
    "        A unique identifier for this class for serialization purposes.\n",
    "\n",
    "        The unique identifier is a list of strings that describes the path to the object.\n",
    "\n",
    "    load_memory_variables(inputs: Dict[str, Any]) → Dict[str, Any][source]\n",
    "\n",
    "        Return history buffer.\n",
    "\n",
    "    classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model\n",
    "\n",
    "    classmethod parse_obj(obj: Any) → Model\n",
    "\n",
    "    classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model\n",
    "\n",
    "    save_context(inputs: Dict[str, Any], outputs: Dict[str, str]) → None\n",
    "\n",
    "        Save context from this conversation to buffer.\n",
    "\n",
    "    classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') → DictStrAny\n",
    "\n",
    "    classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) → unicode\n",
    "\n",
    "    to_json() → Union[SerializedConstructor, SerializedNotImplemented]\n",
    "\n",
    "    to_json_not_implemented() → SerializedNotImplemented\n",
    "\n",
    "    classmethod update_forward_refs(**localns: Any) → None\n",
    "\n",
    "        Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
    "\n",
    "    classmethod validate(value: Any) → Model\n",
    "\n",
    "    property buffer: Union[str, List[langchain.schema.messages.BaseMessage]]\n",
    "\n",
    "        String buffer of memory.\n",
    "\n",
    "    property buffer_as_messages: List[langchain.schema.messages.BaseMessage]\n",
    "\n",
    "        Exposes the buffer as a list of messages in case return_messages is False.\n",
    "\n",
    "    property buffer_as_str: str\n",
    "\n",
    "        Exposes the buffer as a string in case return_messages is True.\n",
    "\n",
    "    property lc_attributes: Dict\n",
    "\n",
    "        List of attribute names that should be included in the serialized kwargs.\n",
    "\n",
    "        These attributes must be accepted by the constructor.\n",
    "\n",
    "    property lc_secrets: Dict[str, str]\n",
    "\n",
    "        A map of constructor argument names to secret ids.\n",
    "\n",
    "        For example,\n",
    "\n",
    "            {“openai_api_key”: “OPENAI_API_KEY”}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain.chat_models.openai.ChatOpenAI\n",
    "\n",
    "class langchain.chat_models.openai.ChatOpenAI\n",
    "\n",
    "    Bases: BaseChatModel\n",
    "\n",
    "    OpenAI Chat large language models API.\n",
    "\n",
    "    To use, you should have the openai python package installed, and the environment variable OPENAI_API_KEY set with your API key.\n",
    "\n",
    "    Any parameters that are valid to be passed to the openai.create call can be passed in, even if not explicitly saved on this class.\n",
    "\n",
    "    Example\n",
    "\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "    Create a new model by parsing and validating input data from keyword arguments.\n",
    "\n",
    "    Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
    "\n",
    "    param cache: Optional[bool] = None\n",
    "\n",
    "        Whether to cache the response.\n",
    "\n",
    "    param callback_manager: Optional[BaseCallbackManager] = None\n",
    "\n",
    "        Callback manager to add to the run trace.\n",
    "\n",
    "    param callbacks: Callbacks = None\n",
    "\n",
    "        Callbacks to add to the run trace.\n",
    "\n",
    "    param max_retries: int = 6\n",
    "\n",
    "        Maximum number of retries to make when generating.\n",
    "\n",
    "    param max_tokens: Optional[int] = None\n",
    "\n",
    "        Maximum number of tokens to generate.\n",
    "\n",
    "    param metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        Metadata to add to the run trace.\n",
    "\n",
    "    param model_kwargs: Dict[str, Any] [Optional]\n",
    "\n",
    "        Holds any model parameters valid for create call not explicitly specified.\n",
    "\n",
    "    param model_name: str = 'gpt-3.5-turbo' (alias 'model')\n",
    "\n",
    "        Model name to use.\n",
    "\n",
    "    param n: int = 1\n",
    "\n",
    "        Number of chat completions to generate for each prompt.\n",
    "\n",
    "    param openai_api_base: Optional[str] = None\n",
    "\n",
    "    param openai_api_key: Optional[str] = None\n",
    "\n",
    "        Base URL path for API requests, leave blank if not using a proxy or service emulator.\n",
    "\n",
    "    param openai_organization: Optional[str] = None\n",
    "\n",
    "    param openai_proxy: Optional[str] = None\n",
    "\n",
    "    param request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n",
    "\n",
    "        Timeout for requests to OpenAI completion API. Default is 600 seconds.\n",
    "\n",
    "    param streaming: bool = False\n",
    "\n",
    "        Whether to stream the results or not.\n",
    "\n",
    "    param tags: Optional[List[str]] = None\n",
    "\n",
    "        Tags to add to the run trace.\n",
    "\n",
    "    param temperature: float = 0.7\n",
    "\n",
    "        What sampling temperature to use.\n",
    "\n",
    "    param tiktoken_model_name: Optional[str] = None\n",
    "\n",
    "        The model name to pass to tiktoken when using this class. Tiktoken is used to count the number of tokens in documents to constrain them to be under a certain limit. By default, when set to None, this will be the same as the embedding model name. However, there are some cases where you may want to use this Embedding class with a model name not supported by tiktoken. This can include when using Azure embeddings or when using one of the many model providers that expose an OpenAI-like API but with different models. In those cases, in order to avoid erroring when tiktoken is called, you can specify a model name to use here.\n",
    "\n",
    "    param verbose: bool [Optional]\n",
    "\n",
    "        Whether to print out response text.\n",
    "\n",
    "    __call__(messages: List[BaseMessage], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → BaseMessage\n",
    "\n",
    "        Call self as a function.\n",
    "\n",
    "    async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) → List[Output]\n",
    "\n",
    "        Default implementation of abatch, which calls ainvoke N times. Subclasses should override this method if they can batch more efficiently.\n",
    "\n",
    "    async agenerate(messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, **kwargs: Any) → LLMResult\n",
    "\n",
    "        Top Level call\n",
    "\n",
    "    async agenerate_prompt(prompts: List[PromptValue], stop: Optional[List[str]] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → LLMResult\n",
    "\n",
    "        Asynchronously pass a sequence of prompts and return model generations.\n",
    "\n",
    "        This method should make use of batched calls for models that expose a batched API.\n",
    "\n",
    "        Use this method when you want to:\n",
    "\n",
    "                take advantage of batched calls,\n",
    "\n",
    "                need more output from the model than just the top generated value,\n",
    "\n",
    "                are building chains that are agnostic to the underlying language model\n",
    "\n",
    "                    type (e.g., pure text completion models vs chat models).\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                prompts – List of PromptValues. A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessages for chat models).\n",
    "\n",
    "                stop – Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings.\n",
    "\n",
    "                callbacks – Callbacks to pass through. Used for executing additional functionality, such as logging or streaming, throughout generation.\n",
    "\n",
    "                **kwargs – Arbitrary additional keyword arguments. These are usually passed to the model provider API call.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            An LLMResult, which contains a list of candidate Generations for each input\n",
    "\n",
    "                prompt and additional model provider-specific output.\n",
    "\n",
    "    async ainvoke(input: Union[PromptValue, str, List[BaseMessage]], config: Optional[RunnableConfig] = None, *, stop: Optional[List[str]] = None, **kwargs: Any) → BaseMessage\n",
    "\n",
    "        Default implementation of ainvoke, which calls invoke in a thread pool. Subclasses should override this method if they can run asynchronously.\n",
    "\n",
    "    async apredict(text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any) → str\n",
    "\n",
    "        Asynchronously pass a string to the model and return a string prediction.\n",
    "\n",
    "        Use this method when calling pure text generation models and only the top\n",
    "\n",
    "            candidate generation is needed.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                text – String input to pass to the model.\n",
    "\n",
    "                stop – Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings.\n",
    "\n",
    "                **kwargs – Arbitrary additional keyword arguments. These are usually passed to the model provider API call.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            Top model prediction as a string.\n",
    "\n",
    "    async apredict_messages(messages: List[BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any) → BaseMessage\n",
    "\n",
    "        Asynchronously pass messages to the model and return a message prediction.\n",
    "\n",
    "        Use this method when calling chat models and only the top\n",
    "\n",
    "            candidate generation is needed.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                messages – A sequence of chat messages corresponding to a single model input.\n",
    "\n",
    "                stop – Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings.\n",
    "\n",
    "                **kwargs – Arbitrary additional keyword arguments. These are usually passed to the model provider API call.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            Top model prediction as a message.\n",
    "\n",
    "    async astream(input: Union[PromptValue, str, List[BaseMessage]], config: Optional[RunnableConfig] = None, *, stop: Optional[List[str]] = None, **kwargs: Any) → AsyncIterator[BaseMessageChunk]\n",
    "\n",
    "        Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output.\n",
    "\n",
    "    async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) → Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]\n",
    "\n",
    "        Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
    "\n",
    "        Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run.\n",
    "\n",
    "        The jsonpatch ops can be applied in order to construct state.\n",
    "\n",
    "    async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) → AsyncIterator[Output]\n",
    "\n",
    "        Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain.prompts.chat.ChatPromptTemplate\n",
    "\n",
    "class langchain.prompts.chat.ChatPromptTemplate\n",
    "\n",
    "    Bases: BaseChatPromptTemplate\n",
    "\n",
    "    A prompt template for chat models.\n",
    "\n",
    "    Use to create flexible templated prompts for chat models.\n",
    "\n",
    "    Examples\n",
    "\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ])\n",
    "\n",
    "    messages = template.format_messages(\n",
    "        name=\"Bob\",\n",
    "        user_input=\"What is your name?\"\n",
    "    )\n",
    "\n",
    "    Create a new model by parsing and validating input data from keyword arguments.\n",
    "\n",
    "    Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
    "\n",
    "    param input_types: Dict[str, Any] [Optional]\n",
    "\n",
    "        A dictionary of the types of the variables the prompt template expects. If not provided, all variables are assumed to be strings.\n",
    "\n",
    "    param input_variables: List[str] [Required]\n",
    "\n",
    "        List of input variables in template messages. Used for validation.\n",
    "\n",
    "    param messages: List[MessageLike] [Required]\n",
    "\n",
    "        List of messages consisting of either message prompt templates or messages.\n",
    "\n",
    "    param output_parser: Optional[BaseOutputParser] = None\n",
    "\n",
    "        How to parse the output of calling an LLM on this formatted prompt.\n",
    "\n",
    "    param partial_variables: Mapping[str, Union[str, Callable[[], str]]] [Optional]\n",
    "\n",
    "    param validate_template: bool = False\n",
    "\n",
    "        Whether or not to try validating the template.\n",
    "\n",
    "    async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) → List[Output]\n",
    "\n",
    "        Default implementation of abatch, which calls ainvoke N times. Subclasses should override this method if they can batch more efficiently.\n",
    "\n",
    "    async ainvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) → Output\n",
    "\n",
    "        Default implementation of ainvoke, which calls invoke in a thread pool. Subclasses should override this method if they can run asynchronously.\n",
    "\n",
    "    append(message: Union[BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate, Tuple[str, str], Tuple[Type, str], str]) → None[source]\n",
    "\n",
    "        Append message to the end of the chat template.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "            message – representation of a message to append.\n",
    "\n",
    "    async astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) → AsyncIterator[Output]\n",
    "\n",
    "        Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output.\n",
    "\n",
    "    async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) → Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]\n",
    "\n",
    "        Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
    "\n",
    "        Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run.\n",
    "\n",
    "        The jsonpatch ops can be applied in order to construct state.\n",
    "\n",
    "    async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) → AsyncIterator[Output]\n",
    "\n",
    "        Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain.prompts.chat.HumanMessagePromptTemplate\n",
    "\n",
    "class langchain.prompts.chat.HumanMessagePromptTemplate\n",
    "\n",
    "    Bases: BaseStringMessagePromptTemplate\n",
    "\n",
    "    Human message prompt template. This is a message sent from the user.\n",
    "\n",
    "    Create a new model by parsing and validating input data from keyword arguments.\n",
    "\n",
    "    Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
    "\n",
    "    param additional_kwargs: dict [Optional]\n",
    "\n",
    "        Additional keyword arguments to pass to the prompt template.\n",
    "\n",
    "    param prompt: langchain.prompts.base.StringPromptTemplate [Required]\n",
    "\n",
    "        String prompt template.\n",
    "\n",
    "    classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model\n",
    "\n",
    "        Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = ‘allow’ was set since it adds all passed values\n",
    "\n",
    "    copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model\n",
    "\n",
    "        Duplicate a model, optionally choose which fields to include, exclude and change.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                include – fields to include in new model\n",
    "\n",
    "                exclude – fields to exclude from new model, as with values this takes precedence over include\n",
    "\n",
    "                update – values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n",
    "\n",
    "                deep – set to True to make a deep copy of the model\n",
    "\n",
    "        Returns\n",
    "\n",
    "            new model instance\n",
    "\n",
    "    dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) → DictStrAny\n",
    "\n",
    "        Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
    "\n",
    "    format(**kwargs: Any) → BaseMessage[source]\n",
    "\n",
    "        Format the prompt template.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "            **kwargs – Keyword arguments to use for formatting.\n",
    "        Returns\n",
    "\n",
    "            Formatted message.\n",
    "\n",
    "    format_messages(**kwargs: Any) → List[BaseMessage]\n",
    "\n",
    "        Format messages from kwargs.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "            **kwargs – Keyword arguments to use for formatting.\n",
    "        Returns\n",
    "\n",
    "            List of BaseMessages.\n",
    "\n",
    "    classmethod from_orm(obj: Any) → Model\n",
    "\n",
    "    classmethod from_template(template: str, template_format: str = 'f-string', **kwargs: Any) → MessagePromptTemplateT\n",
    "\n",
    "        Create a class from a string template.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                template – a template.\n",
    "\n",
    "                template_format – format of the template.\n",
    "\n",
    "                **kwargs – keyword arguments to pass to the constructor.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            A new instance of this class.\n",
    "\n",
    "    classmethod from_template_file(template_file: Union[str, Path], input_variables: List[str], **kwargs: Any) → MessagePromptTemplateT\n",
    "\n",
    "        Create a class from a template file.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "                template_file – path to a template file. String or Path.\n",
    "\n",
    "                input_variables – list of input variables.\n",
    "\n",
    "                **kwargs – keyword arguments to pass to the constructor.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            A new instance of this class.\n",
    "\n",
    "    classmethod get_lc_namespace() → List[str]\n",
    "\n",
    "        Get the namespace of the langchain object.\n",
    "\n",
    "        For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [“langchain”, “llms”, “openai”]\n",
    "\n",
    "    classmethod is_lc_serializable() → bool\n",
    "\n",
    "        Return whether or not the class is serializable.\n",
    "\n",
    "    json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode\n",
    "\n",
    "        Generate a JSON representation of the model, include and exclude arguments as per dict().\n",
    "\n",
    "        encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n",
    "\n",
    "    classmethod lc_id() → List[str]\n",
    "\n",
    "        A unique identifier for this class for serialization purposes.\n",
    "\n",
    "        The unique identifier is a list of strings that describes the path to the object.\n",
    "\n",
    "    classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model\n",
    "\n",
    "    classmethod parse_obj(obj: Any) → Model\n",
    "\n",
    "    classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model\n",
    "\n",
    "    classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') → DictStrAny\n",
    "\n",
    "    classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) → unicode\n",
    "\n",
    "    to_json() → Union[SerializedConstructor, SerializedNotImplemented]\n",
    "\n",
    "    to_json_not_implemented() → SerializedNotImplemented\n",
    "\n",
    "    classmethod update_forward_refs(**localns: Any) → None\n",
    "\n",
    "        Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
    "\n",
    "    classmethod validate(value: Any) → Model\n",
    "\n",
    "    property input_variables: List[str]\n",
    "\n",
    "        Input variables for this prompt template.\n",
    "\n",
    "        Returns\n",
    "\n",
    "            List of input variable names.\n",
    "\n",
    "    property lc_attributes: Dict\n",
    "\n",
    "        List of attribute names that should be included in the serialized kwargs.\n",
    "\n",
    "        These attributes must be accepted by the constructor.\n",
    "\n",
    "    property lc_secrets: Dict[str, str]\n",
    "\n",
    "        A map of constructor argument names to secret ids.\n",
    "\n",
    "        For example,\n",
    "\n",
    "            {“openai_api_key”: “OPENAI_API_KEY”}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the next part of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask app setup\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Environment variable for OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# AI Window Size - Number of moves the AI remembers\n",
    "AI_WINDOW_SIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines declare a flask app, define the api_key from the environment variable, and sets the chat window size for holding the chat history in a rolling memory window containing the last 20 chats.\n",
    "\n",
    "The next section defines a LLM prompt template.  The template contains instructions for the bot followed by the chat history and the user's input for this turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a helpful bot named Bob.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section builds the prompts from the templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template=\"{input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section initializes the chat bot and defines a conversation chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize ChatOpenAI with the desired model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Initialize ConversationChain with the appropriate parameters\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    prompt=chat_prompt, \n",
    "    memory=ConversationBufferWindowMemory(k=AI_WINDOW_SIZE),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section contains the routes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def home():\n",
    "    # Render the chat interface\n",
    "    return render_template('chat.html')\n",
    "\n",
    "@app.route('/initiate_chat', methods=['GET'])\n",
    "def initiate_chat():\n",
    "    # Trigger initial AI response with empty user input\n",
    "    response = conversation.predict(input=\"\")\n",
    "    return jsonify({'reply': response})\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    user_input = request.json['message']\n",
    "    response = conversation.predict(input=user_input)\n",
    "    return jsonify({'reply': response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the system template, I can instruct the bot to behave in defferent ways, depending on the game state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
